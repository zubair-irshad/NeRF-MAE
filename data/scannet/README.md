# ScanNet NeRF Detection Dataset Generation

This folder contains the code for generating a NeRF 3D Object Detection dataset based on [ScanNet](http://www.scan-net.org/). The NeRF implementation used is [Dense Depth Priors NeRF](https://github.com/barbararoessle/dense_depth_priors_nerf). 

Using other NeRF implementation like instant-ngp might be OK, but the number of training views needed is usually higher and the extracted features might demonstrate worse quality.

## Environment
Please follow the instructions in [Dense Depth Priors NeRF](https://github.com/barbararoessle/dense_depth_priors_nerf) to setup the environment. Note that [building the project](https://github.com/barbararoessle/dense_depth_priors_nerf#prepare-scenes) under the `preprocessing` folder is needed.

## Preparation
### ScanNet download
Refer to [ScanNet](http://www.scan-net.org/) for dataset downloading and use the provided [exporter](https://github.com/ScanNet/ScanNet/tree/master/SensReader/python) to extract data from the downloaded `.sens` files.


### Prepare scenes for NeRF training
We provided a streamlined script `prepare_scannet.py` that randomly selects ScanNet scenes from the dataset, samples the camera views, runs COLMAP, and create necessary data for Dense Depth Priors NeRF training. Note that the camera poses are **not aligned** using the alignment information provided by ScanNet.

An example of use, note that GPU for running COLMAP is not necessary:
```bash
python prepare_scannet.py \
--scannet_dir path/to/scannet/root \
--output_dir path/to/output/dir \
--ddp_nerf_dir path/to/ddp_nerf/repo \
--num_scenes 100 \
--num_train_samples 100 \
--num_val_samples 400 \
--gpu 0-7
```

When the script finishes, you should find a `scannet_nerf` folder under the output directory with the following structure:
```
scannet_nerf
|- scene0000_00
|  |- colmap
|  |- test
|  |- train
|  |- config.json
|  |- test_set.csv
|  |- train_set.csv
|  |- transforms_test.json
|  └- transforms_train.json
└-...
```

### Create instance metadata
Run the following to generate `json` files containing instance information, including class labels, AABBs, and OBBs:
```bash
python generate_bbox.py --scene_path scannet/scans/sceneXXXX_XX --output_path dir/for/output
```

Note that OBBs are generated by computing the minimum bounding box of all the vertices belonging to the object, after projecting to the xy plane. You may also use PCA for OBB computation as in Hypersim. Also, the box coordinates are **not aligned**.

The box data at the moment is not filtered. Proceed to NeRF training first to extract the feature.


### NeRF training and feature extraction
A `run_nerf.py` is provided as a drop-in replacement for the original given file [here](https://github.com/barbararoessle/dense_depth_priors_nerf/blob/master/run_nerf.py), which enables extracting density and radiance information from the optimized NeRF model. After replacement, you may train a scene with the command given [here](https://github.com/barbararoessle/dense_depth_priors_nerf#optimize):
```bash
python3 run_nerf.py train \
--scene_id <scene, e.g. scene0710_00> \
--data_dir <directory containing the scenes> \
--depth_prior_network_path <path to depth prior checkpoint> \
--ckpt_dir <path to write checkpoints>
```

When training is finished, extract the feature with
```bash
python3 run_nerf.py extract \
--expname <experiment name> \
--data_dir <directory containing the scenes> \
--ckpt_dir <directory containing the checkpoints> \
--max_res <dimension of the largest side of the 3D feature grid> \
--extract_dir <directory to save the feature> \
--bbox_json <path to the instance metadata file>
```
The `.npz` files under `extract_dir` then contain the features needed in NeRF-RPN. The RGB and density information is in the shape of `(W * L * H, 4)`, which should be reshaped and transposed before used as NeRF-RPN input. A sample processing code:

```python
res = features['resolution']
rgbsigma = rgbsigma['rgbsigma']
rgbsigma = rgbsigma.reshape(res[2], res[1], res[0], -1) # to (H, L, W, 4)
rgbsigma = np.transpose(rgbsigma, (2, 1, 0, 3)) # to (W, L, H, 4)
```

The density at this stage is the raw queried result from NeRF. To transform it to alpha values, use
```python
def density_to_alpha(density):
    activation = np.clip(density, a_min=0, a_max=None)
    return np.clip(1.0 - np.exp(-activation / 100.0), 0.0, 1.0) # 0.01 is used as a dummy distance
```

### Filter instance data
**After you have extracted the NeRF feature as above**, you may further filter the boxes by their class labels and sizes:
```bash
python filter_bbox.py \
--feature_dir path/to/nerf/features \
--obj_json_dir path/to/unfiltered/jsons \
--npy_output_dir ./boxes \
--json_output_dir ./metadata
```

The `.npy` files under `boxes` now contains the OBBs for NeRP RPN training. You can modify the code a bit for it to output AABBs instead. `metadata` contains the `json` files of the objects after filtering.


## Acknowledgements
We appreciate the high-quality indoor dataset made available by [ScanNet](http://www.scan-net.org/), the NeRF implementation from [Dense Depth Priors NeRF](https://github.com/barbararoessle/dense_depth_priors_nerf), as well as the minimum bounding box computation algorithm implementation from [MinimumBoundingBox](https://bitbucket.org/william_rusnack/minimumboundingbox/src/master/).
